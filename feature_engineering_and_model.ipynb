{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec7ef47-7605-46d0-b2ae-d150812bb5ee",
   "metadata": {},
   "source": [
    "# Feature Engineering & Model Developpment\n",
    "\n",
    "Please run eda.ipynb first\n",
    "\n",
    "### Data Loading\n",
    "Load necessary libraries and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f0ada5-d452-4971-9e14-0773f8feda80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, ndcg_score, precision_score, recall_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.api.layers import Concatenate, Dense, Dot, Dropout, Input\n",
    "from keras.api.models import Model, Sequential\n",
    "from keras.api.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc935bd-33a7-4a01-af05-1ae7f9700ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"processed_data/\"\n",
    "\n",
    "big_matrix = pd.read_csv(base_dir + \"big_matrix.csv\")\n",
    "small_matrix = pd.read_csv(base_dir + \"small_matrix.csv\")\n",
    "social_network = pd.read_csv(base_dir + \"social_network.csv\")\n",
    "user_features = pd.read_csv(base_dir + \"user_features.csv\")\n",
    "item_daily_features = pd.read_csv(base_dir + \"item_daily_features.csv\")\n",
    "item_categories = pd.read_csv(base_dir + \"item_categories.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ebc81-855d-4857-907c-7be272788db3",
   "metadata": {},
   "source": [
    "## Categorical Data Preprocessing with MultiLabelBinarizer\n",
    "\n",
    "- Import preprocessing and evaluation tools from sklearn (OneHotEncoder, MultiLabelBinarizer, StandardScaler, PCA, and various metrics).\n",
    "- Initialize a MultiLabelBinarizer to convert lists of multiple labels into binary vectors (each column represents a category, with 1 indicating presence and 0 indicating absence).\n",
    "- Apply the MultiLabelBinarizer to the \"feat\" column of item_categories, which appears to contain lists of categories per video.\n",
    "- Result: a DataFrame where each column is a unique category extracted from all lists, and each row shows a 0 or 1 indicating whether that category is present for a given video_id.\n",
    "- Cast the DataFrame’s columns to int16 to optimize memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81bc760c-30c7-4761-bb2a-30b00854be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "item_categories = pd.DataFrame(mlb.fit_transform(item_categories[\"feat\"]),\n",
    "                  columns=mlb.classes_,\n",
    "                  index=item_categories[\"video_id\"])\n",
    "\n",
    "item_categories.reset_index(drop=True, inplace=True)\n",
    "item_categories[item_categories.columns] = item_categories[item_categories.columns].astype(\"int16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141bd37c-9645-423d-85a2-405654067c35",
   "metadata": {},
   "source": [
    "## Definition of Feature Groups\n",
    "\n",
    "- text_features: list of column names representing categorical or textual variables (e.g., video type, upload type).\n",
    "\n",
    "- integer_feature: list of column names corresponding to integer numerical variables, such as durations, dimensions, identifiers, or video-related statistics (e.g., video duration, view count, number of users).\n",
    "\n",
    "This separation will likely enable different processing depending on the data type (encoding for categorical features, normalization or other transformations for numerical features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ece5c6-7cae-4467-b5e5-9c69e458f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = [\"video_type\", \"upload_type\"]\n",
    "integer_feature = ['video_duration','video_width', 'video_height', 'music_id', 'video_tag_id','show_cnt', 'show_user_num', 'play_cnt', 'play_user_num',\n",
    "                'play_duration', 'complete_play_cnt', 'complete_play_user_num', 'valid_play_cnt', 'valid_play_user_num', 'long_time_play_cnt',\n",
    "                'long_time_play_user_num', 'short_time_play_cnt', 'short_time_play_user_num', 'play_progress']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774369a-c3b3-458f-8178-7940661e3235",
   "metadata": {},
   "source": [
    "## Extraction and Encoding of Daily Video Features\n",
    "\n",
    "- Filter item_daily_features to keep only the latest date (maximum date) for each video_id.\n",
    "    This selects the most recent daily observation for each video.\n",
    "- Select the categorical/textual columns (text_features) from this subset.\n",
    "- Apply One-Hot Encoding (OneHotEncoder) to these text columns, handling unknown categories to avoid errors if new categories appear.\n",
    "- Convert the encoded result to a NumPy array, then to a pandas DataFrame with explicit column names (e.g., video_type_A, upload_type_B).\n",
    "- Reconstruct the final item_daily_features DataFrame by concatenating the numerical columns (integer_feature) with the encoded columns.\n",
    "- Perform a final merge with the binarized categories (item_categories) to create a comprehensive item_features_map table that aggregates all video features.\n",
    "- Explicitly cast all column names to strings and store the list of column names in item_features_columns for easier future access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0faf9-e4d2-400d-bdad-abc2b739281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_daily_features = item_daily_features.loc[item_daily_features.groupby(\"video_id\")[\"date\"].idxmax()].reset_index(drop=True)\n",
    "\n",
    "text_daily_features = item_daily_features[text_features]\n",
    "onehotter = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "onehot_array = onehotter.fit_transform(text_daily_features).toarray()\n",
    "\n",
    "text_daily_features = pd.DataFrame(\n",
    "    onehot_array,\n",
    "    columns=onehotter.get_feature_names_out(text_features),\n",
    "    index=item_daily_features.index)\n",
    "\n",
    "item_daily_features = pd.concat([item_daily_features[integer_feature], text_daily_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68509ebf-fc05-415f-ad45-29f44e0f5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features_map = pd.concat([item_daily_features, item_categories], axis=1)\n",
    "item_features_map.columns = item_features_map.columns.map(str)\n",
    "item_features_columns = item_features_map.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41182144-67c1-4024-999a-12f67e9f7bfe",
   "metadata": {},
   "source": [
    "## Selection and Formatting of User Features\n",
    "\n",
    "- Define a list user_features_columns containing the names of the columns of interest from the user_features DataFrame.\n",
    "These include binary indicators (e.g., is_lowactive_period, is_live_streamer, is_video_author) and several one-hot encoded variables (onehot_featX).\n",
    "\n",
    "- Create a new DataFrame user_features_map that retains only these columns, making a copy to avoid modifying the original.\n",
    "\n",
    "- Cast all columns explicitly to int16 to optimize memory usage, assuming the values are binary flags or other small integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97e937a-ad16-41f0-844c-846822d42c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_columns = [\n",
    "    \"is_lowactive_period\",\"is_live_streamer\", \"is_video_author\",\n",
    "    \"onehot_feat0\", \"onehot_feat1\", \"onehot_feat2\", \"onehot_feat3\",\n",
    "    \"onehot_feat4\", \"onehot_feat5\", \"onehot_feat6\", \"onehot_feat7\",\n",
    "    \"onehot_feat8\", \"onehot_feat9\", \"onehot_feat10\", \"onehot_feat11\",\n",
    "    \"onehot_feat12\", \"onehot_feat13\", \"onehot_feat14\", \"onehot_feat15\",\n",
    "    \"onehot_feat16\", \"onehot_feat17\"\n",
    "]\n",
    "user_features_map = user_features[user_features_columns].copy()\n",
    "\n",
    "user_features_map[user_features_map.columns] = user_features_map[user_features_map.columns].astype(\"int16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770eacb-7bba-4edd-83ea-be25e1654da5",
   "metadata": {},
   "source": [
    "## Preparation of Training and Test Sets for User–Video Interactions\n",
    "\n",
    "- Define the variable `interaction` as 2,500,000, indicating the desired sample size.  \n",
    "- Randomly sample `interaction` rows from `big_matrix` to create `y_train_df`, the training set.  \n",
    "- Extract the unique users (`train_users`) and videos (`train_videos`) present in `y_train_df`.  \n",
    "- Filter `small_matrix` to retain only interactions involving `train_users` and `train_videos`, storing the result in `filtered_small`.  \n",
    "- Randomly sample from `filtered_small` to create `y_test_df`, with size equal to the minimum of `interaction` and `filtered_small.shape[0]`.  \n",
    "- Split the `user_id` and `video_id` columns into separate variables for both sets:  \n",
    "  - Training: `user_ids_train`, `video_ids_train`  \n",
    "  - Testing: `user_ids_test`, `video_ids_test`  \n",
    "- Extract the target values `y_train` and `y_test` from `y_train_df` and `y_test_df`, respectively.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8291ff95-2fd5-41a7-85c0-052e14a39dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERACTION_N = 2500000\n",
    "\n",
    "# Step 1: Sample training interactions from big_matrix\n",
    "y_train_df = big_matrix.sample(n=INTERACTION_N, random_state=42).copy()\n",
    "\n",
    "# Step 2: Get user and video IDs seen in training\n",
    "train_users = set(y_train_df[\"user_id\"])\n",
    "train_videos = set(y_train_df[\"video_id\"])\n",
    "\n",
    "# Step 3: Filter small_matrix to only keep known user-video pairs\n",
    "filtered_small = small_matrix[\n",
    "    small_matrix[\"user_id\"].isin(train_users) &\n",
    "    small_matrix[\"video_id\"].isin(train_videos)\n",
    "].copy()\n",
    "\n",
    "# Optional: Limit test size to INTERACTION_N or smaller\n",
    "y_test_df = filtered_small.sample(n=min(INTERACTION_N, len(filtered_small)), random_state=42)\n",
    "\n",
    "# Step 4: Extract IDs for later use\n",
    "user_ids_train, video_ids_train = y_train_df[\"user_id\"], y_train_df[\"video_id\"]\n",
    "user_ids_test, video_ids_test = y_test_df[\"user_id\"], y_test_df[\"video_id\"]\n",
    "\n",
    "# Step 5: Extract targets\n",
    "y_train = y_train_df[[\"watch_ratio\"]].values\n",
    "y_test = y_test_df[[\"watch_ratio\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea95af-848e-44cc-858e-d0f80a7c962f",
   "metadata": {},
   "source": [
    "## Extraction of User and Video Features for Training and Test Sets\n",
    "\n",
    "- Use `user_ids_train` to select matching user feature vectors from `user_features_map`, storing them in `user_features_train`.  \n",
    "- Perform the same operation for training videos: extract item features from `item_features_map` using `video_ids_train`, storing them in `item_features_train`.  \n",
    "- Repeat this process for the test set with `user_ids_test` and `video_ids_test`, producing `user_features_test` and `item_features_test`.  \n",
    "- Implicitly convert to NumPy arrays via `.values` for seamless integration with machine learning models.  \n",
    "\n",
    "This step prepares the input features required to train and evaluate a user–item interaction prediction model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ec1012c-b965-4a11-9cc5-8994d554fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_train = user_features_map.iloc[user_ids_train].values\n",
    "item_features_train = item_features_map.iloc[video_ids_train].values\n",
    "user_features_test = user_features_map.iloc[user_ids_test].values\n",
    "item_features_test = item_features_map.iloc[video_ids_test].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d34e9ea-73d8-4393-91b1-57046c8aa827",
   "metadata": {},
   "source": [
    "## Normalization of Features and Targets with StandardScaler\n",
    "\n",
    "- Instantiate a `StandardScaler` for item features (`scalerItem`), which standardizes data to zero mean and unit variance.  \n",
    "  - Fit and transform on the training set (`item_features_train`).  \n",
    "  - Transform the test set (`item_features_test`) using the same scaler (no refitting).  \n",
    "- Apply the same process to user features using `scalerUser`.  \n",
    "- Use a separate `StandardScaler` (`scalerTarget`) to normalize target values `y_train` and `y_test`.  \n",
    "\n",
    "This standardization is crucial for most machine learning algorithms to prevent variables with larger scales from dominating.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15458a49-00e7-4594-acac-6ffea0e31aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerItem = StandardScaler()\n",
    "item_features_train = scalerItem.fit_transform(item_features_train)\n",
    "item_features_test = scalerItem.transform(item_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b59d90de-1258-48b7-8690-56978bbd8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerUser = StandardScaler()\n",
    "user_features_train = scalerUser.fit_transform(user_features_train)\n",
    "user_features_test = scalerUser.transform(user_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b92124ca-a887-461d-9892-de2a6e32c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerTarget = StandardScaler()\n",
    "y_train = scalerTarget.fit_transform(y_train)\n",
    "y_test = scalerTarget.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d2f2b-811b-40e8-8aa7-b5757d3dbfcb",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction with PCA (Principal Component Analysis)\n",
    "\n",
    "- Initialize a PCA with `n_components=0.95`, selecting enough components to explain 95% of the data variance.  \n",
    "- Apply PCA to item features:  \n",
    "  - Fit and transform on the training set (`fit_transform`).  \n",
    "  - Transform the test set using the same model (`transform`).  \n",
    "- Apply the same process to user features.  \n",
    "\n",
    "This step reduces the dimensionality of the data while retaining the majority of the information, helping to streamline downstream models and reduce computational cost.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad358401-1853-4c09-b73f-f0d0444a5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_item = PCA(n_components=0.95)\n",
    "item_features_train = pca_item.fit_transform(item_features_train)\n",
    "item_features_test = pca_item.transform(item_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "232cf03c-be45-4d9d-8921-f6173d90d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_user = PCA(n_components=0.95)\n",
    "user_features_train = pca_user.fit_transform(user_features_train)\n",
    "user_features_test = pca_user.transform(user_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055944e-0bfd-434c-8e27-ab4e81d84471",
   "metadata": {},
   "source": [
    "## Construction of a Neural Network to Encode Item Features\n",
    "\n",
    "- Import TensorFlow and Keras libraries, along with necessary layers and classes (`Input`, `Dense`, `Dropout`, `Sequential`, etc.).  \n",
    "- Define input dimensions for users (`user_dim`) and items (`item_dim`) based on the training data.  \n",
    "- Create a Keras input `input_item` corresponding to a vector of size `item_dim` (reduced item features).  \n",
    "- Define a sequential model `item_NN` to encode item features:  \n",
    "  1. Dense layer with 128 units and ReLU activation  \n",
    "  2. Dropout layer with rate = 0.2 for regularization  \n",
    "  3. Dense layer with 64 units and ReLU activation  \n",
    "  4. Final Dense layer with 32 units to produce the **item embedding**, a compact dense representation of the item  \n",
    "- Apply `item_NN` to `input_item` to obtain `item_embedding`.  \n",
    "\n",
    "This architecture learns a dense representation of items from their features, facilitating subsequent interaction modeling.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "244796ee-8522-42ab-8a3a-05c34d31cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dim = user_features_train.shape[1]\n",
    "item_dim = item_features_train.shape[1]\n",
    "\n",
    "input_item = Input(shape=(item_dim,), name=\"item_input\")\n",
    "\n",
    "item_NN = Sequential(\n",
    "    [\n",
    "        Dense(128, activation=\"relu\", name=\"item_x\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(32, activation=\"relu\", name=\"item_embedding\"),\n",
    "    ], name=\"item_NN\"\n",
    ")\n",
    "\n",
    "item_embedding = item_NN(input_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af6acc-9b26-4b55-b9d6-7a5e59ed5ee7",
   "metadata": {},
   "source": [
    "## Construction of a Neural Network to Encode User Features\n",
    "\n",
    "- Create a Keras input `input_user` with shape `user_dim`, representing the user feature vectors.  \n",
    "- Define a sequential model `user_NN` analogous to the item encoder:  \n",
    "  1. Dense layer with 128 units and ReLU activation  \n",
    "  2. Dropout layer with rate 0.2 to reduce overfitting  \n",
    "  3. Dense layer with 64 units and ReLU activation  \n",
    "  4. Final Dense layer with 32 units to produce the **user embedding**, a compact dense representation of user characteristics  \n",
    "- Apply the `user_NN` model to `input_user` to obtain `user_embedding`.  \n",
    "\n",
    "This step learns a dense numerical representation of users, comparable to item embeddings, for modeling their interactions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1545f6a8-b1d4-4190-9a26-62c20f3150ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user = Input(shape=(user_dim,), name=\"user_input\")\n",
    "\n",
    "user_NN = Sequential(\n",
    "    [\n",
    "        Dense(128, activation=\"relu\", name=\"user_x\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(32, activation=\"relu\", name=\"user_embedding\"),\n",
    "    ], name=\"user_NN\"\n",
    ")\n",
    "user_embedding = user_NN(input_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5d368-9718-4c73-967a-5225887baab2",
   "metadata": {},
   "source": [
    "## Construction and Compilation of the Final Prediction Model (“Two-Tower”)\n",
    "\n",
    "Merge embeddings: concatenate the item embedding (item_embedding) and the user embedding (user_embedding) to form a joint representation.\n",
    "\n",
    "Dense network: pass the combined vector through a sequence of dense layers:\n",
    "- Dense layer with 64 neurons and ReLU activation\n",
    "- Dropout layer (rate = 0.2) to mitigate overfitting\n",
    "- Dense layer with 32 neurons and ReLU activation\n",
    "- Output Dense layer with 1 neuron (no activation) to predict a continuous target (e.g., watch ratio)\n",
    "  \n",
    "Model definition: instantiate a Keras Model named Two_Tower_Model, specifying two inputs (input_user, input_item) and the single output tensor.\n",
    "Compilation:\n",
    "- Optimizer: Adam with a learning rate of 0.001\n",
    "- Loss: mean squared error (mse), suitable for regression\n",
    "- Metric: mean absolute error (mae) for performance monitoring\n",
    "Architecture summary: call model.summary() to display the full layer-by-layer structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eba8ad41-8bfd-4a4e-a00e-fe6b6da2b257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Two_Tower_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Two_Tower_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ item_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_NN             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,560</span> │ item_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_NN             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,768</span> │ user_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ item_NN[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ user_NN[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ item_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_NN             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m14,560\u001b[0m │ item_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_NN             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m12,768\u001b[0m │ user_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ item_NN[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ user_NN[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,160\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,601</span> (131.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,601\u001b[0m (131.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,601</span> (131.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,601\u001b[0m (131.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined = Concatenate()([item_embedding, user_embedding])\n",
    "x = Dense(64, activation=\"relu\")(combined)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1)(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=[input_user, input_item], outputs=output, name=\"Two_Tower_Model\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28fde3f-b1aa-4fe0-822c-bbe44da1f7ab",
   "metadata": {},
   "source": [
    "## Model Training with Early Stopping\n",
    "\n",
    "Call the Keras fit method to train the model on the training data:\n",
    "- Inputs: user and item features (user_features_train, item_features_train)\n",
    "- Targets: normalized values y_train\n",
    "- Validation data: provided to monitor performance on a separate set (user_features_test, item_features_test, y_test)\n",
    "\n",
    "Training parameters:\n",
    "- Epochs: 20\n",
    "- Batch size: 512 examples per iteration\n",
    "- EarlyStopping callback enabled to halt training if the loss does not improve for 3 consecutive epochs, with restoration of the best weights achieved.\n",
    "\n",
    "This mechanism prevents overfitting and automatically stops training when the model ceases to make progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cdb38ab-3d6d-44a6-9235-42dd59c4b0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 10ms/step - loss: 1.0394 - mae: 0.3544 - val_loss: 0.5847 - val_mae: 0.2328\n",
      "Epoch 2/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 10ms/step - loss: 0.9141 - mae: 0.3299 - val_loss: 0.5835 - val_mae: 0.2310\n",
      "Epoch 3/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 10ms/step - loss: 0.9248 - mae: 0.3269 - val_loss: 0.5844 - val_mae: 0.2454\n",
      "Epoch 4/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 10ms/step - loss: 0.9620 - mae: 0.3280 - val_loss: 0.5848 - val_mae: 0.2503\n",
      "Epoch 5/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 10ms/step - loss: 0.9394 - mae: 0.3243 - val_loss: 0.5830 - val_mae: 0.2421\n",
      "Epoch 6/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - loss: 0.9284 - mae: 0.3244 - val_loss: 0.5825 - val_mae: 0.2426\n",
      "Epoch 7/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 10ms/step - loss: 0.9476 - mae: 0.3252 - val_loss: 0.5841 - val_mae: 0.2485\n",
      "Epoch 8/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 10ms/step - loss: 0.9623 - mae: 0.3245 - val_loss: 0.5843 - val_mae: 0.2487\n",
      "Epoch 9/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - loss: 0.9668 - mae: 0.3243 - val_loss: 0.5849 - val_mae: 0.2525\n",
      "Epoch 10/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - loss: 0.9069 - mae: 0.3230 - val_loss: 0.5829 - val_mae: 0.2461\n",
      "Epoch 11/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - loss: 0.9480 - mae: 0.3235 - val_loss: 0.5837 - val_mae: 0.2460\n",
      "Epoch 12/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - loss: 0.9694 - mae: 0.3243 - val_loss: 0.5859 - val_mae: 0.2555\n",
      "Epoch 13/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - loss: 0.9351 - mae: 0.3244 - val_loss: 0.5837 - val_mae: 0.2493\n",
      "Epoch 14/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - loss: 0.9520 - mae: 0.3221 - val_loss: 0.5872 - val_mae: 0.2603\n",
      "Epoch 15/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - loss: 0.9491 - mae: 0.3239 - val_loss: 0.5851 - val_mae: 0.2517\n",
      "Epoch 16/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - loss: 0.9512 - mae: 0.3234 - val_loss: 0.5843 - val_mae: 0.2524\n",
      "Epoch 17/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - loss: 0.9071 - mae: 0.3218 - val_loss: 0.5856 - val_mae: 0.2555\n",
      "Epoch 18/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - loss: 0.9467 - mae: 0.3230 - val_loss: 0.5842 - val_mae: 0.2496\n",
      "Epoch 19/20\n",
      "\u001b[1m3907/3907\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - loss: 0.9055 - mae: 0.3220 - val_loss: 0.5882 - val_mae: 0.2648\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=[user_features_train, item_features_train],\n",
    "    y=y_train,\n",
    "    validation_data=([user_features_test, item_features_test], y_test),\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55c49bb1-2cd0-4384-b7eb-981a7c4fd9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"proposed_solution.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355af8ea-fc38-40eb-8fc9-992107f1c16e",
   "metadata": {},
   "source": [
    "## Predictions on the Test Set and Rescaling\n",
    "\n",
    "- Use the loaded model to generate predictions on the user–item pairs in the test set.\n",
    "\n",
    "- The test_predictions are produced in the normalized space (since the target y_train was standardized prior to training).\n",
    "\n",
    "- Apply scalerTarget.inverse_transform() to convert these normalized predictions back to their original scale (denormalization), yielding y_pred.\n",
    "\n",
    "This step enables you to interpret the predictions in the target’s original units (for example, the actual view ratio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d9a6823-1b32-4da7-97c3-9c461f922fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m62500/62500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict([user_features_test, item_features_test])\n",
    "y_pred = scalerTarget.inverse_transform(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da37ec9-3993-4a04-b244-495891df8a0a",
   "metadata": {},
   "source": [
    "## Preparation of Data for Top-K Performance Evaluation\n",
    "\n",
    "- Use a **threshold `threshold = 0.85`** to define video **relevance**: a video is considered relevant (`relevant = 1`) if its `true_watch_ratio` is greater than or equal to 0.85.  \n",
    "- Assign a rank to each prediction per user via `groupby(\"user_id\").cumcount()`. This rank corresponds to the order of predictions in the test set.  \n",
    "- Filter the **top-K recommendations per user** with `results_df[results_df[\"rank\"] < K]`, keeping the first K predictions for each user.  \n",
    "- Construct two matrices:  \n",
    "  - `y_true_matrix`: a binary matrix (0 or 1) indicating for each rank whether the video is actually relevant.  \n",
    "  - `y_score_matrix`: a matrix of predicted scores (`predicted_watch_ratio`) for each user and each rank.  \n",
    "\n",
    "These matrices enable the computation of standard evaluation metrics for top-K recommendation systems, such as **NDCG**, **precision**, **recall**, etc.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05d7f169-753f-471a-ad11-17037a70a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.85\n",
    "K = 25\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"user_id\": user_ids_test,\n",
    "    \"video_id\": video_ids_test,\n",
    "    \"true_watch_ratio\":scalerTarget.inverse_transform(y_test).flatten(),\n",
    "    \"predicted_watch_ratio\": y_pred.flatten()\n",
    "})\n",
    "\n",
    "results_df[\"distance\"] = abs(results_df[\"predicted_watch_ratio\"] - results_df[\"true_watch_ratio\"])\n",
    "\n",
    "results_df[\"relevant\"] = (results_df[\"true_watch_ratio\"] >= threshold).astype(int)\n",
    "\n",
    "results_df[\"rank\"] = results_df.groupby(\"user_id\").cumcount()\n",
    "\n",
    "results_top_k = results_df[results_df[\"rank\"] < K]\n",
    "\n",
    "y_true_matrix = results_top_k.pivot(index=\"user_id\", columns=\"rank\", values=\"relevant\").fillna(0).astype(int)\n",
    "y_score_matrix = results_top_k.pivot(index=\"user_id\", columns=\"rank\", values=\"predicted_watch_ratio\").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd5d85-2311-4043-bef5-eb2c0a0a729f",
   "metadata": {},
   "source": [
    "## Final Model Evaluation and Comparison with a Naive Baseline\n",
    "\n",
    "- Use `model.evaluate(...)` to assess the model on the test set:  \n",
    "  - `test_loss[0]` is the **MSE** on normalized data.  \n",
    "  - `test_loss[1]` is the **MAE** on normalized data.  \n",
    "- Compute the **mean of the training targets** (`mean_train`) on the original scale to serve as a **baseline**.  \n",
    "- Create a constant prediction vector `baseline_pred_orig` filled with `mean_train`, simulating a model that always predicts the training mean.  \n",
    "- Calculate the **baseline MAE** on the original scale using `mean_absolute_error`.  \n",
    "- Print:  \n",
    "  - The training target mean.  \n",
    "  - The model’s test MSE.  \n",
    "  - The baseline MAE (simple reference).  \n",
    "  - The actual model MAE on the original scale (average absolute difference between predictions and true values).  \n",
    "\n",
    "This comparison quantifies the model’s performance against a naive strategy to confirm its added value.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8284b3d-370e-4f8b-8364-f9f4d0476b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m62500/62500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 3ms/step - loss: 0.6026 - mae: 0.2527\n",
      "Mean value (train): 0.9479\n",
      "Test loss (model, scaled): 0.5843\n",
      "Baseline MAE (original scale): 0.4956\n",
      "Model MAE (original scale): 0.4241\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate([user_features_test, item_features_test], y_test)\n",
    "\n",
    "mean_train = scalerTarget.inverse_transform(y_train).mean()\n",
    "\n",
    "baseline_pred_orig = np.full_like(results_df['true_watch_ratio'], fill_value=mean_train)\n",
    "\n",
    "baseline_mae = mean_absolute_error(baseline_pred_orig, results_df['true_watch_ratio'])\n",
    "\n",
    "print(f\"Mean value (train): {mean_train:.4f}\")\n",
    "print(f\"Test loss (model, scaled): {test_loss[0]:.4f}\")\n",
    "print(f\"Baseline MAE (original scale): {baseline_mae:.4f}\")\n",
    "print(f\"Model MAE (original scale): {abs(results_df['predicted_watch_ratio'] - results_df['true_watch_ratio']).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d60b00-a9c9-40d4-8d53-84ca99dd9548",
   "metadata": {},
   "source": [
    "## Calculation of Top-K Metrics: NDCG, Precision, and Recall\n",
    "\n",
    "- **NDCG@K (Normalized Discounted Cumulative Gain)**  \n",
    "  - Assesses the quality of the ranked recommendations by accounting for item positions.  \n",
    "  - A highly relevant video appearing near the top of the list contributes more to the score.  \n",
    "  - Computed via `ndcg_score(...)` on the Top-K score matrices.\n",
    "\n",
    "- **Precision@K**  \n",
    "  - Measures the proportion of recommended items among the top K that are actually relevant.  \n",
    "  - Determined by comparing predicted relevant videos (`y_score_matrix >= RELEVANCE_THRESHOLD`) against ground truth relevance.\n",
    "\n",
    "- **Recall@K**  \n",
    "  - Measures the proportion of all truly relevant videos for a user that appear in the top K recommendations.\n",
    "\n",
    "- **Reporting**  \n",
    "  - Each metric is printed rounded to four decimal places for clear, side-by-side comparison.\n",
    "\n",
    "These Top-K metrics are crucial for evaluating recommendation quality in a ranking context, complementing regression errors by focusing on relevance and ordering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "266600f5-37a7-4bb8-80d6-aa594641b85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@25: 0.8952\n",
      "Precision@25: 0.5266\n",
      "Recall@25: 0.5266\n"
     ]
    }
   ],
   "source": [
    "ndcg = ndcg_score(y_true_matrix.values, y_score_matrix.values, k=K)\n",
    "precision = precision_score(y_true_matrix.values.flatten(), y_score_matrix.values.flatten() >= RELEVANCE_THRESHOLD, average=\"micro\")\n",
    "recall = recall_score(y_true_matrix.values.flatten(), y_score_matrix.values.flatten() >= RELEVANCE_THRESHOLD, average=\"micro\")\n",
    "\n",
    "print(f\"NDCG@{K}: {ndcg:.4f}\")\n",
    "print(f\"Precision@{K}: {precision:.4f}\")\n",
    "print(f\"Recall@{K}: {recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
